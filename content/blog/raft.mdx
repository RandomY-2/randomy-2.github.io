---
slug: raft-consensus-algorithm
title: "Raft: Consensus Algorithm for Managing a Replicated Log"
date: "2026-01-16"
description: "Raft is a consensus algorithm designed to be equivalent to Multi-Paxos but easier to understand through strong leadership, leader election, and clear separation of concerns."
tags: [raft, consensus, distributed-systems, paxos, replication, leader-election]
category: system-papers
---

## Why Raft?

Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members. They are fundamental to building reliable distributed systems—powering everything from distributed databases to configuration management systems.

**Paxos**, the most famous consensus algorithm, has been notoriously difficult to understand and implement correctly. Even though it's been proven correct, many implementations contain bugs, and developers struggle with its subtle complexities.

Raft was designed to provide the same safety and liveness guarantees as Multi-Paxos but with a focus on **understandability**. The Raft paper makes this goal explicit: to decompose consensus into relatively independent subproblems that can be understood, explained, and implemented separately.

**Key innovations of Raft:**

* **Strong leader**: Raft uses a stronger form of leadership than other consensus algorithms. Log entries only flow from the leader to other servers, simplifying log management and achieves **linearlizability**.
* **Leader election**: Raft uses randomized timers to elect leaders, resolving conflicts simply and rapidly.
* **Membership changes**: Raft's mechanism for changing the set of servers in the cluster uses a new joint consensus approach where the majorities of two different configurations overlap during transitions. This allows the cluster to continue operating normally during configuration changes.

---

## Replicated State Machines

Before diving into Raft, let's understand the problem it solves: **replicated state machines**.

State machines on a collection of servers compute identical copies of the same state and can continue operating even if some of the servers are down. They're used to solve a variety of fault tolerance problems in distributed systems.

<div align="center">
![Replicated State Machine](/figures/raft-rsm.png)
</div>

Replicated state machines are typically implemented using a **replicated log**. Each server stores a log containing a series of commands, which its state machine executes in order. Each log contains the same commands in the same order, so each state machine processes the same sequence of commands. Since the state machines are deterministic, each computes the same state and the same sequence of outputs.

**The consensus problem** is keeping the replicated log consistent. The consensus module on a server receives commands from clients and adds them to its log. It communicates with the consensus modules on other servers to ensure that every log eventually contains the same requests in the same order, even if some servers fail.

**Requirements for practical consensus algorithms:**

* **Ensure safety** (never returning an incorrect result) under all non-Byzantine conditions, including network delays, partitions, packet loss, duplication, and reordering.
* **Fully functional** (available) as long as the majority of the servers are operational and can communicate with each other and with clients.
* **Do not depend on timing** to ensure the consistency of the logs (faulty clocks and extreme message delays can, at worst, cause availability problems).
* In the common case, **a command can complete as soon as a majority of the cluster has responded**; a minority of slow servers need not impact overall system performance.

---

## Raft Basics

Raft implements consensus by first electing a distinguished **leader**, then giving the leader complete responsibility for managing the replicated log. The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines.

Raft breaks consensus into three relatively independent subproblems:

* **Leader Election**: A new leader must be chosen when an existing leader fails.
* **Log Replication**: The leader must accept log entries from clients and replicate them across the cluster, forcing the other logs to agree with the leader's own log.
* **Safety**: The key safety property for Raft is the State Machine Safety Property: if any server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log index.

<div align="center">
![Raft Safety](/figures/raft-safety.png)
</div>

### Server States

At any given time, each server is in one of three states: **leader**, **follower**, or **candidate**.

<div align="center">
![Server States](/figures/raft-server-states.png)
</div>

* In normal operation, there is exactly one **leader** and all other nodes are **followers**.
* **Followers** are passive: they issue no requests on their own but simply respond to requests from leaders and candidates.
* The **leader** handles all client requests (if a client contacts a follower, the follower redirects it to the leader).
* The **candidate** state is used to elect a new leader.

### Terms

Raft divides time into **terms** of arbitrary length. Terms are numbered with consecutive integers. Each term begins with an election, in which one or more candidates attempt to become the leader.

<div align="center">
![Terms](/figures/raft-terms.png)
</div>

* If a candidate wins the election, then it serves as the leader for the rest of the term.
* In some situations, an election will result in a split vote. In this case, the term will end with no leader; a new term (with a new election) will begin shortly.
* Raft ensures that there is at most one leader in a given term.

Terms act as a **logical clock** in Raft, allowing servers to detect obsolete information such as stale leaders:

* Each server stores a current term number, which increases monotonically over time.
* Current terms are exchanged whenever servers communicate; if one server's current term is smaller than the other's, it updates to the larger value.
* If a candidate or leader discovers that its term is out of date, it immediately reverts to the follower state.
* If a server receives a request with a stale term number, it rejects the request.

### RPCs

Raft servers communicate through RPCs. The basic consensus algorithm needs two types of RPC calls:

* **RequestVote RPCs**: Initiated by candidates during elections.
* **AppendEntries RPCs**: Initiated by leaders to replicate log entries and to provide heartbeats.

<div align="center">
![Raft Summary](/figures/raft-summary.png)
</div>

---

## Leader Election

Raft uses a **heartbeat mechanism** to trigger leader election.

### How It Works

1. When servers start up, they begin as **followers**.
2. A server remains in the follower state as long as it receives valid RPCs from a leader or candidate.
3. **Leaders send periodic heartbeats** (AppendEntries RPCs that carry no log entries) to all followers to maintain their authority.
4. If a follower receives no communication over a period of time called the **election timeout**, it assumes there is no viable leader and begins an election.

### Election Procedure

When a follower times out:

1. It increments its current term and transitions to the **candidate** state.
2. It votes for itself and issues **RequestVote RPCs** in parallel to each of the other servers in the cluster.
3. A candidate continues until one of three things happens:

   **a) It wins the election:**
   * A candidate wins if it receives votes from a **majority** of the servers for the same term.
   * Each server will vote for at most one candidate in a given term, on a first-come-first-served basis.
   * Once a candidate wins, it becomes the leader and sends heartbeat messages to all other servers.

   **b) Another server establishes itself as the leader:**
   * While waiting for votes, a candidate may receive an **AppendEntries RPC** from another server claiming to be the leader.
   * If the leader's term is at least as large as the candidate's current term, the candidate recognizes the leader as legitimate and returns to follower state.
   * If the term is smaller, the candidate rejects the RPC and continues.

   **c) A period of time goes by with no winner:**
   * If many followers become candidates simultaneously, votes could be split so that no candidate obtains a majority.
   * Each candidate will time out and start a new election by incrementing their term and initiating another round of RequestVote RPCs.

### Randomized Election Timeouts

Raft uses **randomized election timeouts** to ensure that split votes are rare and resolved quickly:

* Election timeouts are chosen randomly from a fixed interval (e.g., 150–300ms).
* This spreads out the servers so that in most cases only a single server will time out; it wins the election and sends heartbeats before any other servers time out.
* Each candidate restarts its randomized election timeout at the start of an election, reducing the likelihood of another split vote.

---

## Log Replication

Once a leader has been elected, it begins servicing client requests.

### Normal Operation

1. Each client request contains a command to be executed by the replicated state machines.
2. The leader **appends the command to its log** as a new entry.
3. It then issues **AppendEntries RPCs in parallel** to each of the other servers to replicate the entry.
4. If the log entry has been **replicated by a majority** of nodes, the leader applies the entry to its state machine and returns the result to the client.
5. If followers crash or run slowly, or if network packets are lost, the leader retries AppendEntries RPCs indefinitely until all followers eventually store all log entries.

<div align="center">
![Raft Log Entry](/figures/raft-log-entry.png)
</div>

### Commitment

The leader decides when it is safe to apply a log entry to the state machines; such an entry is called **committed**. Raft guarantees that committed entries are durable and will eventually be executed by all available state machines.

* A log entry is **committed once the leader has replicated it on a majority of servers**.
* This also commits all **preceding entries** in the leader's log, including entries created by previous leaders.

<div align="center">
![Raft Log Status](/figures/raft-log-status.png)
</div>

### Log Matching Property

Raft maintains the following **Log Matching Property**:

* **Property 1**: If two entries in different logs have the same index and term, then they store the same command.
  * This is ensured by the fact that **only the leader can generate new entries**. A leader creates at most one entry with a given log index in a given term, and log entries never change their position in the log.

* **Property 2**: If two entries in different logs have the same index and term, then the logs are identical in all preceding entries.
  * This is guaranteed by a **consistency check** performed during AppendEntries RPCs. When sending AppendEntries, the leader includes the index and term of the entry immediately preceding the new entries (the **prevLogIndex** and **prevLogTerm**).
  * When a follower receives an AppendEntries RPC, it checks its own log at `prevLogIndex`. If it doesn't find an entry with the same index and term, it refuses the new entries and returns a rejection.
  * This consistency check acts as an induction step: the initial empty state satisfies the Log Matching Property, and the consistency check preserves it whenever logs are extended. Once AppendEntries succeeds, the leader knows that the follower's log is identical to its own log up through the new entries.

### Handling Inconsistencies

In Raft, the leader handles inconsistencies by **forcing the followers' logs to duplicate the leader's own log**. Conflicting entries in follower logs will be overwritten with entries from the leader's log.

To bring a follower's log into consistency, the leader must find the **latest log entry where the two logs agree** (the consensus point), delete any entries in the follower's log after that point, and send the follower all of the leader's entries after that point.

**The consistency check process:**

1. The leader maintains a **nextIndex** for each follower, which is the index of the next log entry it will send to that follower.
2. When a leader first comes to power, it initializes all nextIndex values to the index just after the last one in its log.
3. The leader sends AppendEntries RPCs with `prevLogIndex = nextIndex - 1` and `prevLogTerm` from its log at that index.
4. If a follower's log is inconsistent, the AppendEntries consistency check will fail: the follower doesn't find an entry at `prevLogIndex` with matching term, so it rejects the request.
5. After a rejection, the leader **decrements nextIndex** and retries the AppendEntries RPC with the previous entry's information.
6. This process continues iteratively: the leader keeps decrementing nextIndex and retrying until it finds a point where the logs match.
7. Once the consistency check succeeds, AppendEntries will succeed, removing any conflicting entries in the follower's log and appending entries from the leader's log (if any). The follower's log is now consistent with the leader's and will remain that way for the rest of the term.

---

## Safety

Raft's safety properties ensure that the system never returns an incorrect result.

### Election Restrictions

Raft guarantees that all committed entries from previous terms are present on each new leader from the moment of its election.

**How it works:**

* A candidate must contact a **majority of the cluster** to be elected, which means every committed entry must be present in at least one of those servers.
* If the candidate's log is at least as up-to-date as any other log in that majority, it will hold all the committed entries.
* The **RequestVote RPC** implements this: it includes information about the candidate's log, and the voter denies its vote if its own log is more up-to-date.

**"Up-to-date" definition:** Raft compares the index and term of the last entries in the logs. If the logs have last entries with different terms, the log with the later term is more up-to-date. If they end with the same term, the longer log is more up-to-date.

### Committing Entries from Previous Terms

A leader cannot immediately conclude that an entry from a previous term is committed once it is stored on a majority of servers.

<div align="center">
![Raft Commit Old Log](/figures/raft-commit-old-log.png)
</div>

First, consider the **incorrect** case: the leader is allowed to commit logs from previous terms. Then the above process is:

(a) S1 is the leader in term 2, and the log has already been replicated to S2.

(b) S1 crashes. S5 receives votes from S3, S4, and S5 and becomes the leader, then writes a log entry with index=2 & term=3.

(c) Right after S5 finishes writing, it crashes. S1 is re-elected as leader, with currentTerm = 4. At this moment there are no new client requests. S1 replicates the entry index=2 & term=2 to S3; a majority is reached, and S1 commits this entry (**index = 2 & term = 2**).

(d) At this point, S5 can become leader again by getting votes from S2, S3, S4, and itself, and then replicate **index=2 & term=3** to all other nodes and commit it. Now **the entry at index=2 has been committed twice—once with term=2, and once with term=3**, which violates safety.

(e) In this **correct** situation, before crashing, S1 had replicated its term=4 log entry to a majority of machines, so S5 cannot win the election. This is the correct case where S1 does not fail and replication proceeds correctly.

**Raft's solution:** Raft never commits log entries from previous terms by counting replicas. **Only log entries from the leader's current term are committed by counting replicas**. Once an entry from the current term has been committed, all prior entries are committed indirectly because of the Log Matching Property.

### Safety Arguments

**Leader Completeness Property proof:**

<div align="center">
![Raft Leader Completeness Proof](/figures/raft-leader-completeness-proof.png)
</div>

Suppose the leader for term T (`leader_T`) commits a log entry from its term, but that log entry is not stored by the leader of some future term. Consider the smallest term U > T whose leader (`leader_U`) does not store the entry.

* The committed entry must have been absent from `leader_U`'s log at the time of its election (leaders never delete or overwrite entries).
* `leader_T` replicated the entry on a majority of the cluster, and `leader_U` received votes from a majority of the cluster. Thus, at least one server ("the voter") both accepted the entry from `leader_T` and voted for `leader_U`. The voter is key to reaching a contradiction.

The voter must have accepted the committed entry from `leader_T` before voting for `leader_U`; otherwise it would have rejected the AppendEntries request from `leader_T` (its current term would have been higher than T).

The voter still stored the entry when it voted for `leader_U`, since every intervening leader contained the entry (by assumption), leaders never remove entries, and followers only remove entries if they conflict with the leader.

The voter granted its vote to `leader_U`, so `leader_U`'s log must have been as up-to-date as the voter's. This leads to one of two contradictions:

* **First case**: If the voter and `leader_U` shared the same last log term, then `leader_U`'s log must have been at least as long as the voter's, so its log contained every entry in the voter's log. This is a contradiction, since the voter contained the committed entry and `leader_U` was assumed not to.

* **Second case**: Otherwise, `leader_U`'s last log term must have been larger than the voter's. Moreover, it was larger than T, since the voter's last log term was at least T (it contains the committed entry from term T). The earlier leader that created `leader_U`'s last log entry must have contained the committed entry in its log (by assumption). Then, by the Log Matching Property, `leader_U`'s log must also contain the committed entry, which is a contradiction.

This completes the contradiction. Thus, the leaders of all terms greater than T must contain all entries from term T that are committed in term T. The Log Matching Property guarantees that future leaders will also contain entries that are committed indirectly.

**State Machine Safety Property proof:**

Given the Leader Completeness Property, we can prove the State Machine Safety Property, which states that if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same index.

* At the time a server applies a log entry to its state machine, its log must be identical to the leader's log up through that entry and the entry must be committed.
* Now consider the lowest term in which any server applies a given log index; the Leader Completeness Property guarantees that the leaders for all higher terms will store that same log entry, so servers that apply the index in later terms will apply the same value. Thus, the State Machine Safety Property holds.

Finally, Raft requires servers to apply entries in log index order. Combined with the State Machine Safety Property, this means that all servers will apply exactly the same set of log entries to their state machines, in the same order.

### Follower & Candidate Crash

* If a follower or candidate crashes, then future RequestVote and AppendEntries RPCs sent to it will fail. Raft handles these failures by **retrying indefinitely**.
* If the crashed server restarts, then the RPC will complete successfully.
* If a server crashes after completing an RPC but before responding, then it will receive the same RPC again after it restarts. Raft RPCs are **idempotent**, so this causes no harm.
  * For example, if a follower receives an AppendEntries request that includes log entries already present in its log, it ignores those entries in the new request.

### Timing & Availability

Raft's correctness is **timing-independent**. However, availability (the ability of the system to respond to clients in a timely manner) must inevitably depend on timing.

Leader election is the aspect of Raft where timing is most critical. Raft will be able to elect and maintain a steady leader as long as the system satisfies the following timing requirement:

**broadcastTime ≪ electionTimeout ≪ MTBF**

Where:
* **broadcastTime** is the average time it takes a server to send RPCs in parallel to every server in the cluster and receive their responses.
* **electionTimeout** is the election timeout.
* **MTBF** is the average time between failures for a single server.

**Why these inequalities matter:**

* The broadcast time should be an order of magnitude less than the election timeout so that leaders can reliably send the heartbeat messages required to keep followers from starting elections. Given the randomized approach used for election timeouts, this inequality also makes split votes unlikely.
* The election timeout should be a few orders of magnitude less than MTBF so that the system makes steady progress.

---

## Cluster Membership Changes

Changing the set of servers in the cluster (adding or removing servers) while maintaining safety is challenging. Raft uses a two-phase approach called **joint consensus**.

### The Problem

Any approach where servers switch directly from the old configuration to the new configuration is unsafe. It's impossible to atomically switch all servers at once, so the cluster can potentially split into two independent majorities during the transition.

<div align="center">
![Raft Cluster Change Problem](/figures/raft-cluster-change-problem.png)
</div>

### Joint Consensus

Raft first switches to a transitional configuration called **joint consensus**, which combines both the old and new configurations:

* Log entries are replicated to all servers in **both** configurations.
* Any server from either configuration may serve as leader.
* Agreement (for elections and entry commitment) requires **separate majorities from both the old and new configurations**.

The joint consensus allows individual servers to transition between configurations at different times without compromising safety, and it allows the cluster to continue servicing client requests throughout the configuration change.

### Membership Change Procedure

1. The leader receives a request to change configuration from `C_old` to `C_new`.
2. It stores the configuration for joint consensus (`C_old,new`) as a log entry and replicates it.
3. Once a server adds the new configuration entry to its log, it uses that configuration for all future decisions (a server always uses the latest configuration in its log, regardless of whether it's committed).
4. Once `C_old,new` has been committed, neither `C_old` nor `C_new` can make decisions without approval of the other.
5. The leader then creates a log entry describing `C_new` and replicates it.
6. When the new configuration has been committed under the rules of `C_new`, the old configuration is irrelevant and servers not in the new configuration can be shut down.

### Additional Issues

**New servers catching up:** New servers may not initially store any log entries. To avoid availability gaps, Raft introduces a phase where new servers join as **non-voting members** (the leader replicates log entries to them, but they're not considered for majorities). Once caught up, reconfiguration proceeds.

**Leader not in new configuration:** If the cluster leader is not part of the new configuration, it steps down once it has committed the `C_new` log entry.

**Removed servers disrupting the cluster:** Removed servers can time out and start elections. To prevent this, servers disregard RequestVote RPCs when they believe a current leader exists—specifically, if a server receives a RequestVote RPC within the minimum election timeout of hearing from the current leader, it does not update its term or grant its vote.

---

## Log Compaction

Raft's log grows during normal operation, but in a practical system, it cannot grow without bound. **Snapshotting** allows the system to discard obsolete information from the log.

### How It Works

In snapshotting, the entire current system state is written to a **snapshot** on stable storage, then the entire log up to that point is discarded.

<div align="center">
![Raft Snapshot](/figures/raft-snapshot.png)
</div>

* Each server takes snapshots independently, covering just the committed entries in its log.
* A snapshot includes the **last included index** (the index of the last entry in the log that the snapshot replaces) and the **last included term** (the term of this entry). These are preserved to support AppendEntries consistency checks.
* To enable cluster membership changes, the snapshot also includes the latest configuration in the log.

### Sending Snapshots to Followers

If the leader has already discarded the next log entry it needs to send to a follower, it must send the snapshot instead.

* The leader uses a new RPC called **InstallSnapshot** to send snapshots to followers that are too far behind.
* When a follower receives a snapshot:
  * If the snapshot contains new information, the follower discards its entire log (it's superseded by the snapshot).
  * If the snapshot describes a prefix of the follower's log (due to retransmission), log entries covered by the snapshot are deleted but entries following the snapshot are retained.

### Performance Considerations

**When to snapshot:** One simple strategy is to take a snapshot when the log reaches a fixed size in bytes. If this size is significantly larger than the expected size of a snapshot, the disk bandwidth overhead will be small.

**Writing snapshots without blocking:** Writing a snapshot can take significant time. The solution is to use **copy-on-write techniques** so that new updates can be accepted without impacting the snapshot being written. For example, state machines built with functional data structures naturally support this, or the operating system's copy-on-write support (e.g., fork on Linux) can be used.

---

## Client Interaction

Raft clients send all of their requests to the leader.

### Finding the Leader

When a client first starts up, it connects to a randomly chosen server. If that server is not the leader, it rejects the client's request and supplies information about the most recent leader it has heard from (AppendEntries requests include the network address of the leader). If the leader crashes, client requests will time out; clients then try again with randomly-chosen servers.

### Linearizable Semantics

Raft aims to implement **linearizable semantics** (each operation appears to execute instantaneously, exactly once, at some point between its invocation and its response). However, Raft can execute a command multiple times—for example, if a leader crashes after committing the log entry but before responding to the client.

**Solution:** Clients assign **unique serial numbers** to every command. The state machine tracks the latest serial number processed for each client, along with the associated response. If it receives a command whose serial number has already been executed, it responds immediately without re-executing the request.

### Read-Only Operations

Read-only operations can be handled without writing anything into the log. However, without additional measures, this would risk returning stale data, since the leader responding to the request might have been superseded by a newer leader.

**Raft's solution requires two extra precautions:**

1. **A leader must have the latest information on which entries are committed.** The Leader Completeness Property guarantees that a leader has all committed entries, but at the start of its term, it may not know which those are. To find out, Raft has each leader commit a blank **no-op entry** into the log at the start of its term.

2. **A leader must check whether it has been deposed before processing a read-only request.** Raft handles this by having the leader exchange heartbeat messages with a majority of the cluster before responding to read-only requests.

---

## Summary

Raft achieves consensus by:

1. **Electing a strong leader** who has complete control over log replication.
2. **Replicating log entries** from the leader to followers, ensuring consistency through the Log Matching Property.
3. **Maintaining safety** through election restrictions and careful handling of commitment across terms.

These design choices make Raft easier to understand and implement correctly than Paxos while providing the same safety and liveness guarantees. The strong leader model, clear separation of concerns, and randomized election timeouts all contribute to making Raft a practical consensus algorithm suitable for production systems.
