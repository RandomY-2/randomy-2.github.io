---
slug: ray-osdi2018
title: "Ray: Unifying Simulation, Training, and Serving for RL"
date: "2025-11-13"
description: "Notes and takeaways from reading the Ray OSDI’18 paper."
tags: [ray, distributed-systems, reinforcement-learning, actors, tasks, osdi]
---

## Why RL Needed a New System

Reinforcement learning (RL) workloads look very different from classic supervised learning. Instead of
“train once, serve later” on a static dataset, RL agents **interact** with an environment over time:

* they **simulate** futures to explore long-term effects of actions,
* they **train** policies using data generated by those interactions,
* and they **serve** policies in latency-sensitive control loops.

![Figure 1: RL loop with simulation, training, and serving](/figures/rl-loop.png)

The core object is a **policy**: a mapping from environment state to action. Learning a good policy
alternates between:

1. **Policy evaluation** – run the current policy in the (simulated or real) environment, collect trajectories of
`(state, reward, ...)`.
2. **Policy improvement** – use those trajectories to update the policy, often via (distributed) stochastic gradient
descent.

Ray indicates that This flow creates some new systems requirements:

* **Fine-grained, heterogeneous computations**
  * Duration of a computation can range from milliseconds (e.g., taking an action) to hours
  (e.g., training a complex policy)
  * Training prefers GPUs/TPUs; simulation is often CPU-heavy; serving might require strict millisecond latencies on
  CPUs at the edge.

* **Flexible computation model**
  * RL applications require both stateless and stateful computations
  * **Stateless** (fine-grained simulation, data processing, etc): can be executed on any node in the system, easy
  to achieve load balancing and movement of computation to data
  * **Stateful**: needed for parameter servers, performing repeated computation on GPU-backed data, or running third-party
  simulators that do not expose their state.

* **Dynamic execution**
  * The computation graph itself depends on results: a simulation’s outcome can decide whether we branch, resample,
  or reschedule more work.
  * Tasks finish out of order; the system must react online, not in fixed rounds.

Traditional systems don’t handle these requirements well:

* MapReduce / Spark / Dryad: bulk-synchronous and coarse-grained; great for ETL, not millions of millisecond-level
tasks or interactive serving.
* General task-parallel systems (CIEL, Dask): handle dynamic DAGs, but lack strong support for distributed training
and low-latency serving.
* Deep learning frameworks (TensorFlow, MXNet): optimized for training, but not for massive simulation or policy serving.
* Model serving systems (TensorFlow Serving, Clipper): good for serving, but don’t handle training or simulation.

One *could* glue several systems together, but end-to-end RL apps couple **all three** workloads tightly—simulation,
training, and serving—with stringent latency between them. Stitching multiple systems becomes fragile and hard to operate.

Ray’s answer: design a single distributed system that **natively** supports all three.

---

## Ray in One Sentence

> Ray is a **general-purpose cluster-computing framework** that unifies **simulation, training, and serving** for RL
(and more) by embedding **task-parallel** and **actor-based** computations in one **dynamic execution engine**,
backed by a **sharded global control store** and **bottom-up distributed scheduler**.

At the API level, Ray gives you:

* **Tasks** for stateless, fine-grained parallel work.
* **Actors** for long-lived, stateful components.
* A single **dynamic task graph** tying everything together.

At the system level, Ray’s key ideas are:

* Store all **control state** (lineage, metadata, object locations) in a **sharded, replicated global control store
(GCS)**.
* Keep everything else **stateless** so components can fail and scale independently.
* Use a **bottom-up, two-level scheduler** (local + global) to schedule millions of heterogeneous tasks per second.
* Use an **in-memory distributed object store** (shared memory, Apache Arrow) for low-latency data movement.

---

## Programming Model: Tasks and Actors

![Figure 2: Ray programming model: tasks and actors](/figures/ray-task-vs-actor.png)

### Tasks: Stateless, Fine-Grained Parallelism

A **task** is a remote function invocation on a stateless worker. When you call a Ray remote function, you immediately
get back a **future** (a handle to the eventual result), which you can:

* `ray.get()` to block and retrieve, or
* pass into other remote functions as an argument.

Ray’s tasks are:

* **Stateless & side-effect free**
  Output depends only on input; no hidden state across calls. This gives:

  * **Idempotence** – safe to re-execute on failure.
  * **Fine-grained load balancing** – tasks can run anywhere.
  * **Data locality** – scheduler can place tasks near their inputs.
  * **Cheap recovery** – no checkpoints for intermediate state; just recompute via lineage.

This is ideal for:

* massive simulation (each rollout / environment step as a task),
* pure data-processing stages (feature extraction, preprocessing),
* embarrassingly parallel workloads.

### Actors: Stateful Computation

Some components **must** be stateful: parameter servers, GPU models that keep weights in device memory, or
third-party simulators.

An **actor** in Ray is:

* a **stateful process** with methods you call remotely (each method call returns a future),
* **methods execute serially** on the actor, preserving internal state,
* a handle that can be passed around between tasks/actors.

Advantages:

* **Efficient fine-grained updates** – you mutate internal state in-place instead of serializing and shipping it
for every operation.
* **Wrapping opaque systems** – embed libraries or simulators that aren’t easy to serialize.

Common patterns:

* Parameter servers as actors.
* Per-GPU trainer actors that receive batches and update local models.
* Simulator actors that maintain complex, non-serializable state.

### Extra API Features for Heterogeneity

To handle real RL workloads, Ray extends the basic API in three ways:

* `ray.wait()`
  Wait for the **first k** completed futures instead of all of them. This is crucial when task durations vary
  wildly (e.g., some simulations finish faster than others).

* **Resource annotations**
  Tasks/actors can declare resource requirements (`num_gpus=1`, custom resource tags, etc.) so the scheduler can
  respect device constraints and placement.

* **Nested remote functions**
  Remote functions can invoke other remote functions. Combined with futures and `ray.wait()`, this allows rich
  dynamic computation graphs that evolve at runtime.

---

## Computation Model: A Dynamic Task Graph with State

![Figure 3: Data, control, and stateful edges in Ray’s DAG](/figures/ray-computation-model.png)

Under the hood, Ray models everything as a **dynamic DAG**:

* **Nodes**
  * **Data objects** – immutable values in the object store.
  * **Tasks / actor method invocations** – units of computation.

* **Edges**
  * **Data edges** – from task → object (outputs) and object → task (inputs).
  * **Control edges** – capture nested remote calls: if task `T1` launches `T2`, we add `T1 → T2` as a control edge.
  * **Stateful edges** – the Ray-specific twist for actors. If method `Mi` is followed by `Mj` on the same actor, we
  add `Mi → Mj` as a **stateful edge**.

Those stateful edges encode the implicit dependence on an actor’s internal state. Taken together, all method calls on
a single actor form a **chain** in the DAG.

Why this matters:

* **Stateful Actors embedded cleanly in a stateless DAG**
  Stateful edges help us embed actors in an otherwise stateless task graph, as they capture the implicit data dependency
  between successive method invocations sharing the internal state of an actor.

* **Lineage-based fault tolerance for both tasks and actors**
  The system tracks lineage across data, tasks, and actors. If the system loses some objects, it can easily reconstruct.

---

## Architecture Overview

![Figure 4: Ray architecture: application layer over GCS, scheduler, and object store](/figures/ray-architecture.png)

Ray exposes its programming model via an **application layer**, backed by a **system layer** that ensures scalability
and fault tolerance.

### Application Layer: Driver, Workers, Actors

Three process roles:

* **Driver** – the process running the user’s script.

* **Workers** – stateless processes that execute **tasks**.
  * Tasks are invoked by a driver or another worker.
  * Functions are registered with workers automatically.
  * Workers don’t maintain local state across tasks.

* **Actors** – stateful processes that execute **actor methods**.
  * Explicitly created by drivers or workers.
  * Execute methods serially, with state flowing from one method to the next.

### System Layer Components

The system layer has three main distributed components:

1. **Global Control Store (GCS)**
2. **Bottom-up distributed scheduler**
3. **In-memory distributed object store**

All three are designed to be horizontally scalable and fault tolerant.

---

## Global Control Store (GCS): Source of Truth for Control State

The **GCS** is a **sharded, replicated key-value store with pub/sub functionality** that holds *all* control plane metadata:

* lineage (the DAG),
* object metadata (locations, sizes),
* task metadata,
* cluster state (workers, resources), etc.

### Why a Dedicated GCS?

1. **Lineage-based fault tolerance at scale**

   For coarse-grained systems (Spark, Dryad), a single master can store lineage without being overwhelmed. For Ray’s
   **millions of tiny, dynamic tasks**, that master would become a bottleneck.

   Ray instead **decouples durable lineage storage** (GCS) from everything else. Components write lineage and metadata
   to GCS and otherwise stay stateless.

2. **Low-latency task scheduling and dispatch**

   Many systems combine scheduling and object metadata in a central scheduler. That’s fine when the scheduler isn’t
   on the critical path for every object transfer.

   Ray targets **extremely fine-grained tasks** and communication-intensive patterns like all-reduce. To keep the
   scheduler off the critical path:

   * **Object metadata (locations, sizes)** live in GCS, **not** in the scheduler.
   * Task dispatch (pulling inputs, pushing outputs) uses GCS metadata directly.
   * The scheduler focuses on **where** to run tasks, not on managing every object transfer.

### Stateless Everything Else

Because GCS is the single source of truth for control state, all other components (workers, actors, schedulers, object
store instances) can be **stateless**:

* On failure, they restart and reconstruct needed state from GCS.
* Scaling out is straightforward: just add more replicas; they all coordinate via GCS.
* Debugging / profiling / visualization tools can tap into GCS to observe the entire system.

The paper uses **per-shard chain replication** for GCS to ensure durability and high availability.

---

## Bottom-Up Distributed Scheduler

![Figure 5: Bottom-up scheduling: local schedulers first, then global](/figures/ray-scheduler.png)

Ray’s scheduler has to handle:

* **Millions of tasks per second**
* Tasks that may last a few milliseconds
* Heterogeneous resources and heavy data-movement costs

Classic designs fall short:

* **Centralized schedulers** (Spark, Dryad, CIEL): good locality decisions but too slow (tens of ms) per decision.
* **Fully distributed, work-stealing schedulers** (Sparrow, Canary): scalable, but often assume independent jobs,
ignore data locality, or require a static computation graph.

Ray introduces a **two-level, bottom-up scheduler**:

1. **Local scheduler on each node**

   * Tasks created on a node are first submitted to its **local** scheduler.
   * Local scheduler tries to run tasks locally if:
     * The node isn’t overloaded, and
     * It has the required resources (GPUs, custom resources).
   * If the node is overloaded (queue too long) or lacks resources, it forwards tasks to the global scheduler.

2. **Global scheduler**

   * Receives tasks that can’t be scheduled locally.
   * Chooses a node based on:
     * **Resource constraints** – which nodes have enough GPUs/CPUs/etc.?
     * **Estimated waiting time** – queue size × average task duration.
     * **Estimated input transfer time** – total size of remote inputs / average bandwidth.

   It gets:

   * **Node loads & resource availability** from periodic heartbeats.
   * **Object locations & sizes** from the GCS.
   * Estimates for average task runtime and bandwidth via simple exponential averaging.

Because scheduling is **bottom-up**—local first, then global—most decisions never leave the node. The global
scheduler only sees spillover or special tasks, and can be **replicated** (using GCS as its shared state) if it
becomes a bottleneck.

---

## In-Memory Distributed Object Store

To keep task latencies low, Ray uses an **in-memory object store** on each node:

* Implemented via **shared memory**, so tasks on the same node can share data **zero-copy**.
* Uses **Apache Arrow** as the data format for interoperability and efficient columnar layout.

Key properties:

* **Immutable objects**
  Objects are never mutated in place:
  * Simplifies reasoning: no consistency protocols required.
  * Makes lineage-based reconstruction easy: objects are recomputed, not updated.

* **Local-first accesses**
  * Tasks read/write all their inputs/outputs from the **local** object store.
  * If an input is remote, it is **replicated** to the local store before execution.

* **Replication & eviction**
  * Hot objects get replicated across nodes as different tasks request them.
  * When memory is full, objects are evicted to disk using an LRU policy.

* **No “distributed objects” primitive**
  Each object must fit on a single node. Larger logical structures (huge matrices, trees) are built at the
  application level as **collections of futures**.

This design matches the workloads Ray targets: compute-heavy AI tasks where in-memory data and low-latency
access dominate.

---

## Design Principles I Really Like

Reading the paper, a few design principles stand out as generally useful beyond Ray itself:

* **Centralize control state, decentralize everything else**
  Put lineage and metadata in a **sharded, fault-tolerant GCS**; make all other components **stateless**, restartable,
  and horizontally scalable. This design pattern shows up in many modern data-intensive systems, such as Apache Flink
  and cloud data warehouses like Snowflake, where replicated control/metadata services coordinate largely stateless
  compute workers.

* **Unify stateless and stateful abstractions over a single engine**
  Don’t force users to choose between “task world” and “actor world.” Give them both, backed by the same dynamic
  scheduler and object store.

* **Make fine-grained, heterogeneous tasks first-class**
  Design the scheduler and object store assuming tasks might be millisecond-scale and resource-diverse, not just coarse
  batch jobs.

* **Lineage as the backbone for fault tolerance**
  Treat the task graph as the ground truth for recovery, including actor state via stateful edges, and keep checkpointing
  optional rather than mandatory—similar in spirit to systems like Apache Spark, which reconstruct lost data via
  lineage rather than eagerly materializing all intermediate results.

---

## Closing Thoughts

The Ray paper is less about a single ML algorithm and more about **what the systems stack for modern AI should look like**.
By:

* unifying simulation, training, and serving,
* embracing tasks and actors in one dynamic DAG,
* and restructuring the control plane around a sharded GCS and bottom-up scheduling,

Ray turns RL-style workloads from a “glue together four distributed systems” problem into something first-class and
programmable.

Even for building non-RL applications, the ideas—especially the separation of **control state** from **stateless,
easily-scaled components**—are broadly applicable to modern distributed systems.
