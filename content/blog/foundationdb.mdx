---
slug: foundationdb-newsql-transactional-key-value-store
title: "FoundationDB: A Distributed Transactional Key-Value Store"
date: "2026-01-23"
description: "FoundationDB combines the flexibility and scalability of NoSQL architectures with the power of ACID transactions, using an unbundled architecture and deterministic simulation testing."
tags: [foundationdb, newsql, distributed-systems, transactions, acid, key-value-store, simulation-testing]
category: system-papers
---

## Why FoundationDB?

More than a decade ago, NoSQL storage systems emerged offering ease of application development, making it simple to scale and operate storage systems, offering fault-tolerance and supporting a wide range of data models instead of the traditional rigid relational model.

**However, to achieve scale, these systems sacrificed transactional semantics**, and instead provided eventual consistency, forcing application developers to reason about interleaving of updates from concurrent operations.

**FoundationDB avoids this trade-off** by providing serializable transactions while scaling to handle the large workloads these systems target. It is one of the first systems to combine the flexibility and scalability of NoSQL architectures with the power of ACID transactions—a category now known as **NewSQL**.

### The Modular Approach

Unlike most databases, which bundle together a storage engine, data model, and query language, forcing users to choose all three or none, FoundationDB takes a **modular approach**: it provides a highly scalable, transactional storage engine with a minimal yet carefully chosen set of features.

FoundationDB provides:
* No structured semantics
* No query language
* No data model or schema management
* No secondary indices
* No many other features one normally finds in a transactional database

Offering these features would benefit some applications, but others that do not require them (or do so in a slightly different form) would need to work around them. Instead, the NoSQL model leaves application developers with great flexibility.

FoundationDB focuses on the "lower half" of a database, leaving the rest to its "layers"—stateless applications developed on top to provide various data models and other capabilities. With this, applications that would traditionally require completely different types of storage systems can instead all leverage FoundationDB.

---

## Design Principles

FoundationDB's design is guided by several key principles:

### 1. Divide-and-Conquer (Separation of Concerns)

FoundationDB decouples the transaction management system (write path) from the distributed storage (read path) and scales them independently. Within the transaction management system, processes are assigned various roles representing different aspects of transaction management, including timestamp management, accepting commits, conflict detection, and logging. Furthermore, cluster-wide orchestrating tasks, such as overload control, load balancing, and failure recovery are also divided and serviced by additional heterogeneous roles.

### 2. Make Failure a Common Case

For distributed systems, failure is a norm rather than an exception. In the transaction management system of FoundationDB, we handle all failures through the recovery path: instead of fixing all possible failure scenarios, the transaction system proactively shuts down when it detects a failure. As a result, all failure handling is reduced to a single recovery operation, which becomes a common and well-tested code path. Such error handling strategy is desirable as long as the recovery is quick, and pays dividends by simplifying the normal transaction processing.

### 3. Fail Fast and Recover Fast

To improve availability, FoundationDB strives to minimize Mean-Time-To-Recovery (MTTR), which includes the time to detect a failure, proactively shut down the transaction management system, and recover. In production clusters, the total time is usually less than five seconds.

### 4. Simulation Testing

FoundationDB relies on a randomized, deterministic simulation framework for testing the correctness of its distributed database. Because simulation tests are both efficient and repeatable, they not only expose deep bugs, but also boost developer productivity and the code quality of FoundationDB.

---

## System Interface

FoundationDB exposes operations to read and modify single keys as well as ranges of keys:

* The `get()` and `set()` operations read and write a single key-value pair, respectively.
* For ranges, `getRange()` returns a sorted list of keys and their values within the given range.
* `clear()` deletes all keys-value pairs within a range or starting with a certain key prefix.

**Transaction model:**
* An FoundationDB transaction observes and modifies a snapshot of the database at a certain version and changes are applied to the underlying database only when the transaction commits.
* A transaction's writes (i.e., `set()` and `clear()` calls) are buffered by the FoundationDB client until the final `commit()` call.
* Read-your-write semantics are preserved by combining results from database look-ups with uncommitted writes of the transaction.

**Limits:**
* Key and value sizes are limited to 10 KB and 100 KB respectively for better performance.
* Transaction size is limited to 10 MB, including the size of all written keys and values as well as the size of all keys in read or write conflict ranges that are explicitly specified.

---

## Architecture

<div align="center">
![FoundationDB Architecture](/figures/fdb-architecture.png)
</div>

An FoundationDB cluster has a **control plane** for managing critical system metadata and cluster-wide orchestration, and a **data plane** for transaction processing and data storage.

### Control Plane

The control plane is responsible for persisting critical system metadata, i.e., the configuration of transaction systems, on **Coordinators**.

* **Coordinators** form a disk Paxos group and select a singleton **ClusterController**.
* The **ClusterController** monitors all servers in the cluster and recruits three singleton processes:
  * **Sequencer** - assigns read and commit versions to transactions
  * **DataDistributor** - responsible for monitoring failures and balancing data among StorageServers
  * **Ratekeeper** - provides overload protection for the cluster

These singleton processes are re-recruited if they fail or crash.

### Data Plane

FoundationDB targets OLTP workloads that are read-mostly, read and write a small set of keys, have low contention, and require scalability.

FoundationDB chooses an **unbundled architecture**:

1. **Distributed Transaction Management System (TS)** - performs in-memory transaction processing
2. **Log System (LS)** - stores Write-Ahead-Log (WAL) for TS
3. **Distributed Storage System (SS)** - used for storing data and servicing reads

**Transaction System Components:**
* The **Sequencer** assigns a read version and a commit version to each transaction and, for historical reasons, also recruits Proxies, Resolvers, and LogServers.
* **Proxies** offer MVCC read versions to clients and orchestrate transaction commits.
* **Resolvers** check for conflicts between transactions.
* **LogServers** act as replicated, sharded, distributed persistent queues, where each queue stores WAL data for a StorageServer.

**Storage System:**
* The SS consists of a number of **StorageServers** for serving client reads, where each StorageServer stores a set of data shards, i.e., contiguous key ranges.
* StorageServers are the majority of processes in the system, and together they form a distributed B-tree.
* Currently, the storage engine on each StorageServer is a modified version of SQLite, with enhancements that make range clears faster, defer deletion to a background task, and add support for asynchronous programming.

### Read-Write Separation and Scaling

FoundationDB's design is decoupled; processes are assigned different roles (e.g., Coordinators, StorageServers, Sequencer), and the database scales by expanding the number of processes for each role.

This separates the scaling of client reads from client writes (i.e., transaction commits):

* Because clients directly issue reads to sharded StorageServers, **reads scale linearly** with the number of StorageServers.
* Similarly, **writes are scaled** by adding more processes to Proxies, Resolvers, and LogServers in TS and LS.
* For this reason, MVCC data is stored in the SS, which is different from Deuteronomy that stores the MVCC data in TS.
* The singletons (e.g., ClusterController and Sequencer) and Coordinators on the control plane are not performance bottlenecks, because they only perform limited metadata operations.

### Bootstrapping

All user data and most of the system metadata (keys that start with `0xFF` prefix) are stored in StorageServers. The metadata about StorageServers is persisted in LogServers, and the configuration of LS (i.e., information about LogServers) is stored in all Coordinators.

Using Coordinators as a disk Paxos group, servers attempt to become the ClusterController if one does not exist. The newly elected ClusterController recruits a new Sequencer, which reads the configuration of old LS stored in Coordinators and spawns a new TS and LS. From the old LS, Proxies recover system metadata, including information about all StorageServers. The Sequencer waits until the new TS finishes recovery, and then writes the new LS configuration to all Coordinators. At this time, the new transaction system becomes ready to accept client transactions.

### Reconfiguration

Whenever there is a failure in the TS or LS, or a database configuration change, a reconfiguration process brings the transaction management system to a new configuration, i.e., a clean state.

Specifically, the Sequencer process monitors the health of Proxies, Resolvers, and LogServers. If any one of the monitored processes fails or the database configuration changes, the Sequencer process terminates. The ClusterController will detect the Sequencer failure event, then recruit a new Sequencer, which follows the above bootstrapping process to spawn the new TS and LS instance. In this way, transaction processing is divided into **epochs**, where each epoch represents a generation of the transaction management system with its unique Sequencer process.

---

## Transaction Management

### End-to-End Transaction Processing

A client transaction starts by contacting one of the Proxies to obtain a **read version** (i.e., a timestamp). The Proxy then asks the Sequencer for a read version that is guaranteed to be no less than any previously issued transaction commit version, and this read version is sent back to the client. Then the client may issue multiple reads to StorageServers and obtain values at that specific read version. Client writes are buffered locally without contacting the cluster.

At commit time, the client sends the transaction data, including the read and write sets (i.e., key ranges), to one of the Proxies and waits for a commit or abort response from the Proxy. If the transaction cannot commit, the client may choose to restart the transaction from the beginning again.

**A Proxy commits a client transaction in three steps:**

1. **Obtain commit version**: The Proxy contacts the Sequencer to obtain a commit version that is larger than any existing read versions or commit versions. The Sequencer chooses the commit version by advancing it at a rate of one million versions per second.

2. **Conflict detection**: The Proxy sends the transaction information to range-partitioned Resolvers, which implement FoundationDB's optimistic concurrency control by checking for read-write conflicts. If all Resolvers return with no conflict, the transaction can proceed to the final commit stage. Otherwise, the Proxy marks the transaction as aborted.

3. **Logging**: Committed transactions are sent to a set of LogServers for persistence. A transaction is considered committed after all designated LogServers have replied to the Proxy, which reports the committed version to the Sequencer (to ensure that later transactions' read versions are after this commit) and then replies to the client. At the same time, StorageServers continuously pull mutation logs from LogServers and apply committed updates to disks.

**Read-only transactions and snapshot reads:**
* A read-only transaction in FoundationDB is both serializable (happens at the read version) and performant (thanks to the MVCC), and the client can commit these transactions locally without contacting the database. This is particularly important because the majority of transactions are read-only.
* Snapshot reads in FoundationDB selectively relax the isolation property of a transaction by reducing conflicts, i.e., concurrent writes will not conflict with snapshot reads.

### Supporting Strict Serializability

FoundationDB implements **Serializable Snapshot Isolation (SSI)** by combining **Optimistic Concurrency Control (OCC)** with **Multi-Version Concurrency Control (MVCC)**.

A transaction `Tx` gets both its read version and commit version from Sequencer, where:
* The read version is guaranteed to be no less than any committed version when `Tx` starts
* The commit version is larger than any existing read or commit versions

This commit version defines a serial history for transactions and serves as **Log Sequence Number (LSN)**. Because `Tx` observes the results of all previous committed transactions, FoundationDB achieves strict serializability.

To ensure there is no gaps between LSNs, the Sequencer returns the previous commit version (i.e., previous LSN) with commit version. A Proxy sends both LSN and previous LSN to Resolvers and LogServers so that they can serially process transactions in the order of LSNs. Similarly, StorageServers pull log data from LogServers in increasing LSNs as well.

**Conflict Detection Algorithm:**

<div align="center">
![FoundationDB Conflict Detection Algorithm](/figures/fdb-conflict-algorithm.png)
</div>

Each Resolver maintains a history `lastCommit` of recently modified key ranges by committed transactions, and their corresponding commit versions. The commit request for `Tx` comprises two sets:
* A set of modified key ranges `Rw`
* A set of read key ranges `Rr` (where a single key is converted to a single key range)

The read set is checked against the modified key ranges of concurrent committed transactions, which prevents phantom reads. If there are no read-write conflicts, Resolvers admit the transaction for commit and update the list of modified key ranges with the write set. For snapshot reads, they are not included in the set `Rr`. In practice, `lastCommit` is represented as a version-augmented probabilistic SkipList.

**Key design choice:** Unlike write-snapshot isolation, which assigns the timestamp after checking `Rr`, FoundationDB decides the commit version before the conflict detection. This allows FoundationDB to efficiently batch-process both version assignments and conflict detection. One single-threaded Resolver can easily handle 280K TPS (each transaction reads a random key range and writes another random key range).

The entire key space is divided among Resolvers so that the above read-write conflict detection algorithm may be performed in parallel. A transaction can commit only when all Resolvers admit the transaction. Otherwise, the transaction is aborted.

**False positives:** It is possible that an aborted transaction is admitted by a subset of Resolvers, and they have already updated their history of `lastCommit`, which may cause other transactions to conflict (i.e., a false positive). In practice, this has not been an issue for production workloads, because:
* Transactions' key ranges usually fall into one Resolver
* Because the modified keys expire after the MVCC window, the false positives are limited to only happen within the short MVCC window time (i.e., 5 seconds)
* The key ranges of Resolvers are dynamically adjusted to balance their loads

**OCC benefits:** The OCC design of FoundationDB avoids the complicated logic of acquiring and releasing (logical) locks, which greatly simplifies interactions between the TS and the SS. The price paid for this simplification is to keep the recent commit history in Resolvers. Another drawback is not guaranteeing that transactions will commit, a challenge for OCC. Because of the nature of multi-tenant production workload, the transaction conflict rate is very low (less than 1%) and OCC works well. If a conflict happens, the client can simply restart the transaction.

### Logging Protocol

<div align="center">
![FoundationDB Logging](/figures/fdb-logging.png)
</div>

After a Proxy decides to commit a transaction, the log message is broadcast to all LogServers:

1. The Proxy first consults its in-memory shard map to determine the StorageServers responsible for the modified key range.
2. Then the Proxy attaches StorageServer tags to the mutation, where each tag has a preferred LogServer for storage.
3. The mutation is only sent to the preferred LogServers and an additional LogServer to meet the replication requirements. All other LogServers receive an empty message body. (in the above example, LogServer 1 and 4 are preferred locations, and LogServer 3 is for replication)
4. The log message header includes both LSN and the previous LSN obtained from the Sequencer, as well as the known committed version (KCV) of this Proxy.
5. LogServers reply to the Proxy once the log data is made durable, and the Proxy updates its KCV to the LSN if all replica LogServers have replied and this LSN is larger than the current KCV.

**Background log shipping:** Shipping the redo log from the LS to the SS is not a part of the commit path and is performed in the background. In FoundationDB, StorageServers aggressively fetch redo logs from LogServers before they are durable on the LS, allowing very low latency for serving multi-version reads.

Because the log data is already durable on LogServers, StorageServers can buffer updates in memory and only persist batches of data to disks with a longer delay, thus improving I/O efficiency by coalescing the updates.

### Transaction System Recovery

Traditional database systems often employ the ARIES recovery protocol, which depends on a write-ahead log (WAL) and periodic, coarse-grained checkpoints. During the recovery, the system processes redo log records from the last checkpoint by re-applying them to the relevant data pages.

**FoundationDB's approach:** In FoundationDB, the recovery is purposely made very cheap—there is no checkpoint, and no need to re-apply redo or undo log during recovery. This is possible because of a great simplifying principle of traditional databases: the redo log processing is the same as normal log forward path. In FoundationDB, StorageServers always pull logs from LogServers and apply them in the background, which essentially decouples redo log processing from the recovery.

The recovery process starts by detecting a failure, recruits a new transaction system, and ends when old LogServers are no longer needed. The new transaction system can even accept transactions before all the data on old LogServers is processed, because the recovery only needs to find out the end of redo log and re-applying the log is performed asynchronously by StorageServers.

**Recovery steps for each epoch:**

1. The Sequencer reads the previous transaction system states (i.e. configurations of the transaction system) from Coordinators and locks the coordinated states to prevent another Sequencer process from recovering at the same time.
2. The Sequencer recovers previous transaction system states, including the information about all older LogServers, stops these LogServers from accepting transactions, and recruits a new set of Proxies, Resolvers, and LogServers.
3. After previous LogServers are stopped and a new transaction system is recruited, the Sequencer then writes the coordinated states with current transaction system information.
4. Finally, the Sequencer accepts new transaction commits.

**Recovery Version (RV) determination:**

<div align="center">
![FoundationDB KCV](/figures/fdb-kcv.png)
</div>

The essence of the recovery of old LogServers is to determine the end of redo log, i.e., a **Recovery Version (RV)**. Rolling back undo log is essentially discarding any data after RV in the old LogServers and StorageServers.

Recall that a Proxy request to LogServers piggybacks its KCV, the maximum LSN that this Proxy has committed. Each LogServer keeps the maximum KCV received and a **Durable Version (DV)**, which is the maximum persisted LSN. During a recovery, the Sequencer attempts to stop all `m` old LogServers, where each response contains the DV and KCV on that LogServer. Assume the replication degree for LogServers is `k`. Once the Sequencer has received more than `m - k` replies, the Sequencer knows the previous epoch has committed transactions up to the maximum of all KCVs, which becomes the previous epoch's end version (PEV). All data before this version has been fully replicated. For current epoch, its start version is `PEV + 1` and the Sequencer chooses the minimum of all DVs to be the RV. Logs in the range of `[PEV + 1, RV]` are copied from previous epoch's LogServers to the current ones, for healing the replication degree in case of LogServer failures. The overhead of copying this range is very small because it only contains a few seconds' log data.

When Sequencer accepts new transactions, the first is a special recovery transaction that informs StorageServers the RV so that they can roll back any data larger than RV. The current FoundationDB storage engine consists of an unversioned SQLite B-tree and in-memory multi-versioned redo log data. Only mutations leaving the MVCC window (i.e., committed data) are written to SQLite. The rollback is simply discarding in-memory multi-versioned data in StorageServers. Then StorageServers pull any data larger than version `PEV` from new LogServers.

---

## Replication

FoundationDB uses a combination of various replication strategies for different data to tolerate `f` failures:

### Metadata Replication

System metadata of the control plane is stored on Coordinators using **Active Disk Paxos**. As long as a quorum (i.e., majority) of Coordinators are live, this metadata can be recovered.

### Log Replication

When a Proxy writes logs to LogServers, each sharded log record is synchronously replicated on `k = f + 1` LogServers. Only when all `k` have replied with successful persistence can the Proxy send back the commit response to the client. Failure of a LogServer results in a transaction system recovery.

### Storage Replication

Every shard, i.e., a key range, is asynchronously replicated to `k = f + 1` StorageServers, which is called a **team**. A StorageServer usually hosts a number of shards so that its data is evenly distributed across many teams. A failure of a StorageServer triggers DataDistributor to move data from teams containing the failed process to other healthy teams.

**Key insight:** Unlike most similar systems, FoundationDB does not rely on quorums to mask failures, but rather tries to eagerly detect and recover from them by reconfiguring the system. This allows us to achieve the same level of fault tolerance with significantly fewer resources: FoundationDB can tolerate `f` failures with only `f + 1` (rather than `2f + 1`) replicas. This approach is best suited for deployments in a local or metro area.

---

## Other Optimizations

### Transaction Batching

To amortize the cost of committing transactions, the Proxy groups multiple transactions received from clients into one batch, asks for a single commit version from the Sequencer, and sends the batch to Resolvers for conflict detection. The Proxy then writes committed transactions in the batch to LogServers. The transaction batching reduces the number of calls to obtain a commit version from the Sequencer, allowing Proxies to commit tens of thousands of transactions per second without significantly impacting the Sequencer's performance. Additionally, the batching degree is adjusted dynamically, shrinking when the system is lightly loaded to improve commit latency, and increasing when the system is busy in order to sustain high commit throughput.

### Atomic Operations

FoundationDB supports atomic operations such as atomic add, bitwise "and" operation, compare-and-clear, and set-version-stamp. These atomic operations enable a transaction to write a data item without reading its value, saving a round-trip time to the StorageServers. Atomic operations also eliminate read-write conflicts with other atomic operations on the same data item (only write-read conflicts can still happen). This makes atomic operations ideal for operating on keys that are frequently modified, such as a key-value pair used as a counter.

The set-version-stamp operation is another interesting optimization, which sets part of the key or part of the value to be the transaction's commit version. This enables client applications to later read back the commit version and has been used to improve the performance of client-side caching.

---

## Geo-Replication and Failover

The main challenge of providing high availability during region failures is the trade-off of performance and consistency:

* **Synchronous cross-region replication** provides strong consistency, but pays the cost of high latency.
* **Asynchronous replication** reduces latency by only persisting in the primary region, but may lose data when performing a region failover.

FoundationDB can be configured to perform either synchronous or asynchronous cross-region replication.

### Multi-Availability Zone Design

There is a third possibility that leverages multiple availability zones within the same region, and provides a high level of failure independence, notwithstanding the unlikely event of a complete region outage. This design:

* Always avoids cross-region write latencies, as for asynchronous replication
* Provides full transaction durability, like synchronous replication, so long as there is no simultaneous failure of multiple availability zones in a region
* Can do rapid and completely automatic failover between regions
* Can be manually failed-over with the same guarantees as asynchronous replication (providing A, C, and I of ACID but potentially exhibiting a Durability failure) in the unlikely case of a simultaneous total region failure
* Only requires full replicas of the database in the primary and secondary regions' main availability zones, not multiple replicas per region

<div align="center">
![FoundationDB Two-Region Replication](/figures/fdb-replication.png)
</div>

**Layout:**

* Both regions have a data center (DC) as well as one or more satellite sites.
* Satellites are located in close proximity to the DC (in the same region) but are failure independent.
* The resource requirements from satellites are insignificant as they only need to store log replicas (i.e., a suffix of the redo logs), while data centers host LS, SS, and (when primary) the TS.
* Control plane replicas (i.e., coordinators) are deployed across three or more failure domains (in some deployments utilizing an additional region), usually with at least 9 replicas.
* Relying on majority quorums allows the control plane to tolerate one site (data center/satellite) failure and an additional replica failure.

**Typical deployment:**
* One of the data centers (DC1), configured with a higher priority compared to DC2, is designated as the primary (its region is denoted as the primary region, accordingly) and contains the full TS, LS, and SS.
* DC2 in the secondary region has replicas of data with its own LS and SS.
* Reads can be served from storage replicas at both primary and secondary data centers (consistent reads do require obtaining a read version from the primary data center).
* All client writes are forwarded to the primary region and processed by Proxies in DC1, then synchronously persisted onto LogServers in DC1 and one or both satellite sites in the primary region (depending on the configuration), avoiding the cross-region WAN latency.
* The updates are then asynchronously replicated to DC2, where they are stored on multiple LS servers and eventually spread out to multiple StorageServers.
* **LogRouters** implement a special type of FoundationDB role that facilitates cross-region data transfer. They were created to avoid redundant cross-region transfers of the same information. Instead, LogRouters transfer each log entry across WAN only once, and then deliver it to all relevant LS servers locally in DC2.

**Satellite configuration options:**
* **(1) Synchronously storing updates on all log replicas at the satellite with the highest priority in the region.** In this case, if the satellite fails, another satellite with the next priority is recruited for the task.
* **(2) Synchronously storing updates on all replicas of two satellites with the highest priorities in the region.** In this case, if a satellite fails, it can be similarly replaced with a different satellite of lower priority, or, if none available, fall back to option (1) of using a single satellite.
* **(3) Similar to option (2) but FoundationDB only waits for one of the two satellites to make the mutations durable before considering a commit successful.**

In all cases, if no satellites are available, only the LogServers in DC1 are used. With option 1 and 3, a single site (data center or satellite) failure can be tolerated, in addition to one or more LogServer failures (since the remaining locations have multiple log replicas). With option 2, two site failures in addition to one or more LogServer failures can be tolerated. In options 1 and 2, however, commit latency is sensitive to the tail network latencies between the primary data center and its satellites, which means that option 3 is usually faster.

**Failover:** When DC1 in the primary region suddenly becomes unavailable, the cluster (with the help of Coordinators) detects the failure and starts a new transaction management system in DC2. New LogServers are recruited from satellites in the secondary region, in accordance with the region's replication policy. During recovery, LogRouters in DC2 may need to fetch the last few seconds' data from primary satellites, which, due to the asynchronous replication, may not have made it to DC2 prior to the failover. After the recovery, if the failures in Region 1 are healed and its replication policy can again be met, the cluster will automatically fail-back to have DC1 as the primary data center due to its higher priority. Alternatively, a different secondary region can be recruited.

---

## Simulation Testing

<div align="center">
![FoundationDB Simulation Testing](/figures/fdb-simulation-testing.png)
</div>

FoundationDB was built from the ground up to make this testing approach possible. All database code is deterministic; accordingly multithreaded concurrency is avoided (instead, one database node is deployed per core).

**The simulator process:**
* All sources of nondeterminism and communication are abstracted, including network, disk, time, and pseudo random number generator.
* FoundationDB is written in **Flow**, a novel syntactic extension to C++ adding async/await-like concurrency primitives.
* Flow provides the Actor programming model that abstracts various actions of the FoundationDB server process into a number of actors that are scheduled by the Flow runtime library.
* The simulator process is able to spawn multiple FoundationDB servers that communicate with each other through a simulated network in a single discrete-event simulation.
* The production implementation is a simple shim to the relevant system calls.

**Workloads:**
* The simulator runs multiple workloads (also written in Flow) that communicate with simulated FoundationDB servers through the simulated network.
* These workloads include fault injection instructions, mock applications, database configuration changes, and direct internal database functionality invocations.
* Workloads are composable to exercise various features and are reused to construct comprehensive test cases.

**Test oracles and fault injection:**
* FoundationDB uses a variety of **test oracles** to detect failures in simulation.
* The FoundationDB simulator **injects** machine, rack, and data-center level fail-stop failures and reboots, a variety of network faults, partitions, and latency problems, disk behavior (e.g. the corruption of unsynchronized writes when machines reboot), and randomizes event times.

This rigorous testing in simulation makes FoundationDB extremely stable, and allows its developers to introduce new features and releases in a rapid cadence.

---

## Lessons Learned

### Architecture Design

The divide-and-conquer design principle has proven to be an enabling force for flexible cloud deployment, making the database extensible as well as performant:

* Separating the transaction system from the storage layer enables greater flexibility in placing and scaling compute and storage resources independently. Operators are free to place heterogeneous roles of FoundationDB on different server instance types, optimizing for performance and costs.

* The decoupling design makes it possible to extend the database functionality, such as ongoing work of supporting RocksDB as a drop-in replacement for the current SQLite engine.

* Many of the recent performance improvements are specializing functionality as dedicated roles, e.g., separating DataDistributor and Ratekeeper from Sequencer, adding storage cache, dividing Proxies into get-read-version proxy and commit proxy. This design pattern successfully allows new features and capabilities to be added frequently.

### Simulation Testing

Simulation testing has enabled FoundationDB to maintain a very high development velocity with a small team by shortening the latency between a bug being introduced and a bug being found, and by allowing deterministic reproduction of issues.

### Fast Recovery

Fast recovery is not only useful for improving availability, but also greatly simplifies the software upgrades and configuration changes and makes them faster. Traditional wisdom of upgrading a distributed system is to perform rolling upgrades so that rollback is possible when something goes wrong. The duration of rolling upgrades can last from hours to days. In contrast, FoundationDB upgrades can be performed by restarting all processes at the same time, which usually finishes within a few seconds. Because this upgrade path has been extensively tested in simulation, all upgrades in production clusters are performed in this way. Additionally, this upgrade path simplifies protocol compatibility between different versions—we only need to make sure on-disk data is compatible. There is no need to ensure the compatibility of RPC protocols between different software versions.

### 5-Second MVCC Window

FoundationDB chooses a 5-second MVCC window to limit the memory usage of the transaction system and storage servers, because the multi-version data is stored in the memory of Resolvers and StorageServers, which in turn restricts transaction sizes. From experience, this 5s window is long enough for the majority of OLTP use cases. If a transaction exceeds the time limit, it is often the case that the client application is doing something inefficient, e.g., issuing reads one by one instead of parallel reads. As a result, exceeding the time limit often exposes inefficiency in the application.

For some transactions that may span more than 5s, many can be divided into smaller transactions. For instance, the continuous backup process of FoundationDB will scan through the key space and create snapshots of key ranges. Because of the 5s limit, the scanning process is divided into a number of smaller ranges so that each range can be performed within 5s. In fact, this is a common pattern: one transaction creates a number of jobs and each job can be further divided or executed in a transaction. FoundationDB has implemented such a pattern in an abstraction called TaskBucket and the backup system heavily depends on it.
